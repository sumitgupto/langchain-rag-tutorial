{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG - REACT - TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_model():\n",
    "    from jproperties import Properties \n",
    "    configs = Properties() \n",
    "  \n",
    "    with open('sample.properties', 'rb') as read_prop: \n",
    "        configs.load(read_prop) \n",
    "      \n",
    "    prop_view = configs.items() \n",
    "    for item in prop_view:\n",
    "        if item[0] == 'embed_model':\n",
    "            print(item[0], ':', item[1].data)\n",
    "            embed_model = item[1].data\n",
    "    return embed_model\n",
    "\n",
    "def get_llm_model():\n",
    "    from jproperties import Properties \n",
    "    configs = Properties() \n",
    "  \n",
    "    with open('sample.properties', 'rb') as read_prop: \n",
    "        configs.load(read_prop) \n",
    "    prop_view = configs.items() \n",
    "    for item in prop_view: \n",
    "        if item[0] == 'llm_model':\n",
    "            print(item[0], ':', item[1].data)\n",
    "            llm_model = item[1].data\n",
    "    return llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
    "    #embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  \n",
    "    embed_model = get_embed_model()\n",
    "    embeddings = OpenAIEmbeddings(model=embed_model, dimensions=1536)  \n",
    "\n",
    "    # Create a Chroma vector store using the provided text chunks and embedding model, \n",
    "    # configuring it to save data to the specified directory \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory) \n",
    "\n",
    "    return vector_store  # Return the created vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate the same embedding model used during creation\n",
    "    #embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536) \n",
    "    embed_model = get_embed_model()\n",
    "    embeddings = OpenAIEmbeddings(model=embed_model, dimensions=1536) \n",
    "\n",
    "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings) \n",
    "\n",
    "    return vector_store  # Return the loaded vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pdf document into LangChain \n",
    "#Load the documents\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "DATA_PATH = \"mydata\"\n",
    "loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size, chunk_overlap): #make this 512 anc see u shall not get the result\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=500, chunk_overlap=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_model : text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=5):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm_model = get_llm_model()\n",
    "    llm = ChatOpenAI(model=llm_model, temperature=0)\n",
    "    #llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.invoke(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_model : text-embedding-3-small\n",
      "llm_model : gpt-3.5-turbo\n",
      "The default maximum weight supported in lbs is 40000 lbs.\n"
     ]
    }
   ],
   "source": [
    "# Asking questions\n",
    "#question = 'Does WMi support json interfaces?'\n",
    "#question = 'Who is the current prime minister of the UK?'\n",
    "#question = 'what is the answer to 3 ** 3.9'\n",
    "#question = 'Tell me about Napoleon Bonaparte early life'\n",
    "#question = 'how to enable \"Putaway To Empty Non-Replenishable Dynamic Active Locations\" in Manhattan Active Supply Chain'\n",
    "#question = 'how to enable \"Putaway To Empty Non-Replenishable Dynamic Active Locations\" in Manhattan Active速 Warehouse Management?'\n",
    "#question = 'Is the feature \"Creation of Multiple Process Needs for an Item\" in Manhattan Active Supply Chain, enabled by default?'\n",
    "#question = 'Is the feature \"Creation of Multiple Process Needs for an Item\" in Manhattan Active速 Warehouse Management, enabled by default?'\n",
    "#question = 'What is the maximum weight and volume supported for transportaion order size in Manhattan Active TM?'\n",
    "#question = 'what is the default maximum weight and volume supported?'\n",
    "question = 'what is the default maximum weight supported in lbs?'\n",
    "#question = 'Explain transportation order alerts and missing fields'\n",
    "\n",
    "vector_store = load_embeddings_chroma()\n",
    "answer = ask_and_get_answer(vector_store, question)\n",
    "#print(answer)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_model : gpt-3.5-turbo\n",
      "I don't have a specific number to multiply by 2 based on the context provided.\n"
     ]
    }
   ],
   "source": [
    "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
    "question = 'Multiply that number by 2.'\n",
    "answer = ask_and_get_answer(vector_store, question)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_model : gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains \n",
    "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
    "\n",
    "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "llm_model = get_llm_model()\n",
    "#llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0) \n",
    "llm = ChatOpenAI(model_name=llm_model, temperature=0)  \n",
    "\n",
    "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})  \n",
    "\n",
    "\n",
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Link the ChatGPT LLM\n",
    "    retriever=retriever,  # Link the vector store based retriever\n",
    "    memory=memory,  # Link the conversation memory\n",
    "    chain_type='stuff',  # Specify the chain type\n",
    "    verbose=False  # Set to True to enable verbose logging for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to ask questions\n",
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_model : text-embedding-3-small\n",
      "The default maximum weight supported in lbs is 40000 lbs.\n"
     ]
    }
   ],
   "source": [
    "# Asking questions\n",
    "#question = 'Does WMi support json interfaces?'\n",
    "#question = 'Who is the current prime minister of the UK?'\n",
    "#question = 'what is the answer to 3 ** 3.9'\n",
    "#question = 'Tell me about Napoleon Bonaparte early life'\n",
    "#question = 'how to enable \"Putaway To Empty Non-Replenishable Dynamic Active Locations\" in Manhattan Active Supply Chain'\n",
    "#question = 'how to enable \"Putaway To Empty Non-Replenishable Dynamic Active Locations\" in Manhattan Active速 Warehouse Management?'\n",
    "#question = 'Is the feature \"Creation of Multiple Process Needs for an Item\" in Manhattan Active Supply Chain, enabled by default?'\n",
    "#question = 'Is the feature \"Creation of Multiple Process Needs for an Item\" in Manhattan Active速 Warehouse Management, enabled by default?'\n",
    "#question = 'What is the maximum weight and volume supported for transportaion order size in Manhattan Active TM?'\n",
    "#question = 'what is the default maximum weight and volume supported?'\n",
    "question = 'what is the default maximum weight supported in lbs?'\n",
    "#question = 'Explain transportation order alerts and missing fields'\n",
    "\n",
    "vector_store = load_embeddings_chroma()\n",
    "\n",
    "result = ask_question(question, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying the default maximum weight of 40000 lbs by 10 is 400,000 lbs.\n"
     ]
    }
   ],
   "source": [
    "question = 'Multiply that number by 10.'\n",
    "result = ask_question(question, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have the specific calculation for that, as the information provided is related to payload data and not mathematical calculations.\n"
     ]
    }
   ],
   "source": [
    "question = 'Devide the result by 40.'\n",
    "result = ask_question(question, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='what is the default maximum weight supported in lbs?'\n",
      "content='The default maximum weight supported in lbs is 40000 lbs.'\n",
      "content='Multiply that number by 10.'\n",
      "content='The result of multiplying the default maximum weight of 40000 lbs by 10 is 400,000 lbs.'\n",
      "content='Devide the result by 40.'\n",
      "content=\"I don't have the specific calculation for that, as the information provided is related to payload data and not mathematical calculations.\"\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
