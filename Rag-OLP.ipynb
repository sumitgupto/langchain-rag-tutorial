{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG - REACT - TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## # of Documents ##################\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "#from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain.chains import LLMMathChain, LLMChain\n",
    "import spacy\n",
    "\n",
    "# Create tools for the agent\n",
    "################### first custom tool #############\n",
    "\n",
    "#create the WMI serach tool\n",
    "#Load the documents\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "DATA_PATH = \"mydata\"\n",
    "loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#Split the documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "#text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "## This does not work yet ####\n",
    "\"\"\"\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "doc = nlp(docs)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##Get teh chunks\n",
    "#chunks = text_splitter.create_documents([docs])\n",
    "#print(f'Now you have {len(chunks)}')\n",
    "## End of getting chinks\n",
    "\n",
    "'''\n",
    "print(\"####### FIRST 10 docs ############\")\n",
    "for i in range(10):\n",
    "    print(\"\\n\")\n",
    "    print(documents[i])\n",
    "\n",
    "print(\"####### LAST 10 docs ############\")\n",
    "x = documents[::-1]\n",
    "i = 0\n",
    "while (i < 10):\n",
    "    print(\"\\n\")\n",
    "    print(x[i])\n",
    "    i+= 1\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"############## # of Documents ##################\")\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiatlise the Embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm importing the necessary libraries and initializing the Pinecone client\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "pc = pinecone.Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... Done\n"
     ]
    }
   ],
   "source": [
    "# deleting all indexes\n",
    "indexes = pc.list_indexes().names()\n",
    "for i in indexes:\n",
    "    print('Deleting all indexes ... ', end='')\n",
    "    pc.delete_index(i)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index wmsearch-openai\n",
      "Index created! üòä\n"
     ]
    }
   ],
   "source": [
    "# creating an index\n",
    "from pinecone import ServerlessSpec\n",
    "#from pinecone import PodSpec\n",
    "index_name = 'wmsearch-openai'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f'Creating index {index_name}')\n",
    "    \n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "\n",
    "    ### Code for GCP starter ####\n",
    "    \"\"\"\n",
    "    # creating a new index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=PodSpec(\n",
    "            environment='gcp-starter'\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print('Index created! üòä')\n",
    "else:\n",
    "    print(f'Index {index_name} already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Calculate Embedding cost  #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 60972\n",
      "Embedding Cost in USD: 0.001219\n"
     ]
    }
   ],
   "source": [
    "# calculate embedding cost using tiktoken\n",
    "def calculate_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    # print(f'Total Tokens: {total_tokens}')\n",
    "    # print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
    "    return total_tokens, total_tokens / 1000 * 0.00002\n",
    "\n",
    "tokens, embedding_cost = calculate_embedding_cost(documents) ##documents == chunks\n",
    "\n",
    "print(f'Total Tokens: {tokens}')\n",
    "print(f'Embedding Cost in USD: {tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 640}},\n",
       " 'total_vector_count': 640}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "# inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "vectorstore = Pinecone.from_documents(documents, embeddings, index_name=index_name)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "\n",
    "class WMSearchTool(BaseTool):\n",
    "    name = \"WM search\"\n",
    "    description = \"useful when you need to answer questions about WM product\"\n",
    "\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        #store = vectorstore.as_retriever()\n",
    "        store = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 5})  \n",
    "        #docs = store.get_relevant_documents(query)\n",
    "        docs = store.invoke(query)\n",
    "        text_list = [doc.page_content for doc in docs]\n",
    "        return \"\\n\".join(text_list)\n",
    "\n",
    "    '''\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "    '''\n",
    "\n",
    "################## custom tool end ####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Python REPL Tool (for executing Python code)\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm=OpenAI()\n",
    "\n",
    "python_repl = PythonREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,\n",
    "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm=OpenAI()\n",
    "\n",
    "# 2. Wikipedia Tool (for searching Wikipedia)\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
    ")\n",
    "\n",
    "# 3. DuckDuckGo Search Tool (for general web searches)\n",
    "search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo Search',\n",
    "    func=search.run,\n",
    "    description='Useful for when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
    ")\n",
    "\n",
    "#Another math tool as REPL not working as expected\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "problem_chain = LLMMathChain.from_llm(llm=llm)\n",
    "math_tool = Tool.from_function(name=\"Calculator\",\n",
    "                func=problem_chain.run,\n",
    "                description=\"Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "#llm=OpenAI()\n",
    "\n",
    "# Initialize the ChatOpenAI model (gpt-4-turbo-preview) with a temperature of 0. Utilize gpt-3.5-turbo if you use the free plan\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo', temperature=0)\n",
    "\n",
    "#Answer the following questions in GERMAN as best you can.\n",
    "# Define a template for answering questions\n",
    "template = '''\n",
    "Answer the following questions as best you can.\n",
    "if you don't know the answer, say \"I don't know\".\n",
    "Answer in Bengali\n",
    "Questions: {q}\n",
    "'''\n",
    "\n",
    "#Answer in Kannada Language. # just for fun\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# Pull the react prompt from the hub\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "\n",
    "\n",
    "# displaying information about the react prompt\n",
    "# print(type(prompt))\n",
    "# print(prompt.input_variables)\n",
    "# print(prompt.template)\n",
    "\n",
    "\n",
    "# Combine the tools into a list\n",
    "tools = [WMSearchTool(), math_tool, python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
    "\n",
    "# Create a react agent with the ChatOpenAI model, tools, and prompt\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Initialize the AgentExecutor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    memory=memory,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find specific information about the maximum weight and volume supported for transportation order size in Manhattan Active TM.\n",
      "\n",
      "Action: WM search\n",
      "\n",
      "Action Input: \"maximum weight and volume supported for transportation order size in Manhattan Active TM\"\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID ee228d16-dc90-456b-ba7c-60366202b491, but expected {'tool'} run.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mManhattan Active TM allows users to subscribe to and receive notifications when the planned sizes of transportation orders exceed\n",
      "\n",
      "the maximum capacity allowed within the solution. Currently, the default maximum weight of 40000 lbs. (standard weight UOM) and\n",
      "\n",
      "maximum volume of 40000 cuft (standard volume UOM) are set. If the user needs to reset, then modify the rule using the sample\n",
      "\n",
      "rule ‚Äú(transportationOrder.totalWeight > 40000) or (transportationOrder.totalVolume > 40000)‚Äù, which refers to the Alert Type\n",
      "\n",
      "‚ÄúTOSizeMaximumCapacityViolation.‚Äù\n",
      "\n",
      "Enablement: This feature is controlled by a configuration that is disabled by default.\n",
      "\n",
      "Related:\n",
      "\n",
      "Transportation Management Alerts\n",
      "\n",
      "Non-Event-Based Notification\n",
      "\n",
      "Manhattan Active TM supports the following shipment non-event-based notifications:\n",
      "\n",
      "Shipment in tendered status but not accepted by the carrier.\n",
      "\n",
      "Shipment in planned status but not tendered.\n",
      "\n",
      "Shipment missed the scheduled appointment.\n",
      "Manhattan Active TM allows users to subscribe to and receive notifications when the planned sizes of transportation orders exceed\n",
      "\n",
      "the maximum capacity allowed within the solution. Currently, the default maximum weight of 40000 lbs. (standard weight UOM) and\n",
      "\n",
      "maximum volume of 40000 cuft (standard volume UOM) are set. If the user needs to reset, then modify the rule using the sample\n",
      "\n",
      "rule ‚Äú(transportationOrder.totalWeight > 40000) or (transportationOrder.totalVolume > 40000)‚Äù, which refers to the Alert Type\n",
      "\n",
      "‚ÄúTOSizeMaximumCapacityViolation.‚Äù\n",
      "\n",
      "Enablement: This feature is controlled by a configuration that is disabled by default.\n",
      "\n",
      "Related:\n",
      "\n",
      "Transportation Management Alerts\n",
      "\n",
      "Non-Event-Based Notification\n",
      "\n",
      "Manhattan Active TM supports the following shipment non-event-based notifications:\n",
      "\n",
      "Shipment in tendered status but not accepted by the carrier.\n",
      "\n",
      "Shipment in planned status but not tendered.\n",
      "\n",
      "Shipment missed the scheduled appointment.\n",
      "Transportation Order Alerts\n",
      "\n",
      "Transportation Order Missing Fields\n",
      "\n",
      "Manhattan Active TM allows users to subscribe to and receive notifications when the following specific fields on the transportation\n",
      "\n",
      "order are missing: Origin Facility, Destination Facility, Designated Resources, Pickup Start End Date Time, Delivery Start End Date\n",
      "\n",
      "Time, Product Class, Climate Control, Billing Method, Total weight, and Total volume.\n",
      "\n",
      "Copyright ¬© 2024 Manhattan Associates, Inc.\n",
      "\n",
      "Page 26\n",
      "\n",
      "Manhattan Active¬Æ Supply Chain\n",
      "\n",
      "Release Notes\n",
      "\n",
      "If additional fields or attributes need to be considered, a condition in the Event Action rule must be added using the sample rule\n",
      "\n",
      "‚ÄútransportationOrder.extended.extendedattribute.isNull(),‚Äù which refers to the Alert Type ‚ÄúMissingTOFieldsActionAlert.‚Äù\n",
      "\n",
      "Enablement: This feature is controlled by a configuration that is disabled by default.\n",
      "\n",
      "Related:\n",
      "\n",
      "Transportation Management Alerts\n",
      "\n",
      "Transportation Order Sizes greater than Maximum Capacity Allowed\n",
      "Transportation Order Alerts\n",
      "\n",
      "Transportation Order Missing Fields\n",
      "\n",
      "Manhattan Active TM allows users to subscribe to and receive notifications when the following specific fields on the transportation\n",
      "\n",
      "order are missing: Origin Facility, Destination Facility, Designated Resources, Pickup Start End Date Time, Delivery Start End Date\n",
      "\n",
      "Time, Product Class, Climate Control, Billing Method, Total weight, and Total volume.\n",
      "\n",
      "Copyright ¬© 2024 Manhattan Associates, Inc.\n",
      "\n",
      "Page 26\n",
      "\n",
      "Manhattan Active¬Æ Supply Chain\n",
      "\n",
      "Release Notes\n",
      "\n",
      "If additional fields or attributes need to be considered, a condition in the Event Action rule must be added using the sample rule\n",
      "\n",
      "‚ÄútransportationOrder.extended.extendedattribute.isNull(),‚Äù which refers to the Alert Type ‚ÄúMissingTOFieldsActionAlert.‚Äù\n",
      "\n",
      "Enablement: This feature is controlled by a configuration that is disabled by default.\n",
      "\n",
      "Related:\n",
      "\n",
      "Transportation Management Alerts\n",
      "\n",
      "Transportation Order Sizes greater than Maximum Capacity Allowed\n",
      "Page 30\n",
      "\n",
      "Manhattan Active¬Æ Supply Chain\n",
      "\n",
      "A I I S n n N - - L l L i i i n n n e e e e v d a i l t i o d f a S t h i i o p n p t e o d d Q i u s a p n l t a i y t t y h e e r r o r \" S h i p p e d l a r g e r t h a n o r d e r q u a n t i t y \" ; i f s h i p p e d q u a n t i t y e n t e r e d b y t h e u s e r ( C h a n g e i n t h e s h i p p e d q u a n t i t y ) e x c e e d s t h e c u r r e n t S h i p p e d q u a n t i t y + R e m a i n i n g q u a n t i t y t o b e s h i p p e d o n t h e P O L i n e . O v e r s h i p p e r c e n t a g e f r o m t h e r e c e i v i n g p a r a m e t e r s i s c o n s i d e r e d w h i l e e v a l u a t i n g t h e s h i p p e d q u a n t i t y . i . e I f O v e r s h i p p e r c e n t a g e i s s e t t o b e 5 0 , t h e n 5 0 % o f o r d e r q u a n t i t y c a n b e o v e r s h i p p e d o n t h e A S N l i n e s .\n",
      "\n",
      "Copyright ¬© 2024 Manhattan Associates, Inc.\n",
      "\n",
      "Inbound\n",
      "\n",
      "Page 31\n",
      "\n",
      "Manhattan Active¬Æ Supply Chain\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "\n",
      "Final Answer: ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡¶π‡¶æ‡¶ü‡¶® ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡¶ø‡¶≠ ‡¶ü‡¶ø‡¶è‡¶Æ-‡¶è ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶®‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶∞‡ßç‡¶¨‡¶æ‡¶ß‡¶ø‡¶ï ‡¶ì‡¶ú‡¶® ‡ß™‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶™‡¶æ‡¶â‡¶®‡ßç‡¶° ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶∞‡ßç‡¶¨‡¶æ‡¶ß‡¶ø‡¶ï ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡ß™‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ò‡¶®‡¶´‡ßÅ‡¶ü ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶® ‡¶ï‡¶∞‡ßá‡•§\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡¶π‡¶æ‡¶ü‡¶® ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡¶ø‡¶≠ ‡¶ü‡¶ø‡¶è‡¶Æ-‡¶è ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶®‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶∞‡ßç‡¶¨‡¶æ‡¶ß‡¶ø‡¶ï ‡¶ì‡¶ú‡¶® ‡ß™‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶™‡¶æ‡¶â‡¶®‡ßç‡¶° ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶∞‡ßç‡¶¨‡¶æ‡¶ß‡¶ø‡¶ï ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡ß™‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ò‡¶®‡¶´‡ßÅ‡¶ü ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶® ‡¶ï‡¶∞‡ßá‡•§\n"
     ]
    }
   ],
   "source": [
    "#question = 'Does WMi support json interfaces?'\n",
    "#question = 'Who is the current prime minister of the UK?'\n",
    "#question = 'what is the answer to 3 ** 3.9'\n",
    "#question = 'Tell me about Napoleon Bonaparte early life'\n",
    "#question = 'how to enable \"Putaway To Empty Non-Replenishable Dynamic Active Locations\" in Manhattan Active Supply Chain'\n",
    "#question = 'how to enable \"Putaway To Empty Non-Replenishable Dynamic Active Locations\" in Manhattan Active¬Æ Warehouse Management?'\n",
    "#question = 'Is the feature \"Creation of Multiple Process Needs for an Item\" in Manhattan Active Supply Chain, enabled by default?'\n",
    "#question = 'Is the feature \"Creation of Multiple Process Needs for an Item\" in Manhattan Active¬Æ Warehouse Management, enabled by default?'\n",
    "question = 'What is the maximum weight and volume supported for transportaion order size in Manhattan Active TM?'\n",
    "#question = 'what is the default maximum weight and volume supported?'\n",
    "#question = 'what is the default maximum weight supported in lbs?'\n",
    "#question = 'Explain transportation order alerts and missing fields'\n",
    "\n",
    "output = agent_executor.invoke({\n",
    "    'input': prompt_template.format(q=question)\n",
    "})\n",
    "\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\\nAnswer the following questions as best you can.\\nif you don\\'t know the answer, say \"I don\\'t know\".\\nAnswer in Bengali\\nQuestions: What is the maximum weight and volume supported for transportaion order size in Manhattan Active TM?\\n'\n",
      "content='‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡¶π‡¶æ‡¶ü‡¶® ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡¶ø‡¶≠ ‡¶ü‡¶ø‡¶è‡¶Æ-‡¶è ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶®‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶∞‡ßç‡¶¨‡¶æ‡¶ß‡¶ø‡¶ï ‡¶ì‡¶ú‡¶® ‡ß™‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶™‡¶æ‡¶â‡¶®‡ßç‡¶° ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶∞‡ßç‡¶¨‡¶æ‡¶ß‡¶ø‡¶ï ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡ß™‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ò‡¶®‡¶´‡ßÅ‡¶ü ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶® ‡¶ï‡¶∞‡ßá‡•§'\n"
     ]
    }
   ],
   "source": [
    "for item in output['chat_history']:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
